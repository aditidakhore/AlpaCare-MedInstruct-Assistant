{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44dbf40",
   "metadata": {},
   "source": [
    "#  Hardware Requirement\n",
    "\n",
    "This notebook requires a high-RAM **A100 GPU**. Please ensure you are using a Google Colab Pro+ subscription and have selected this runtime environment. [cite_start]Fine-tuning a 7B parameter model, even with quantization, is memory-intensive. [cite: 66, 67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c113397",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clone the repository and navigate into the project directory\n",
    "!git clone https://github.com/aditidakhore/AlpaCare-MedInstruct-Assistant.git\n",
    "%cd AlpaCare-MedInstruct-Assistant\n",
    "\n",
    "# Install pinned dependencies from the requirements.txt file\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c089c94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b539be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 1. Configure Quantization\n",
    "# This object tells the transformer's library how to load the model in 4-bit.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 2. Load the Quantized Model and Tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Load the base model with our quantization settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer, which translates text to numbers the model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# A standard practice for models that don't have a specific padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Important for causal LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e670dcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. Prepare the model for PEFT\n",
    "# Enable gradient checkpointing for more memory savings\n",
    "model.gradient_checkpointing_enable() \n",
    "# Prepare the quantized model for PEFT training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 2. Create LoRA Configuration\n",
    "# These are the settings for our LoRA \"adapters\"\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 3. Apply LoRA to the Model\n",
    "# This combines our base model with the LoRA config to create a trainable PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 4. Print the percentage of trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_param} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.4f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969dc67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# 1. Define Training Arguments\n",
    "# These are the settings that control the training process.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./alpacare-finetuned-model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True, # Use mixed precision for training\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 2. Prepare Dataset for Colab Demonstration\n",
    "# Load the full prepared dataset from the data_loader script\n",
    "from data_loader import load_and_prepare_dataset\n",
    "prepared_datasets = load_and_prepare_dataset()\n",
    "\n",
    "# --- FOR COLAB DEMONSTRATION ---\n",
    "# To ensure the notebook runs in a reasonable time, we'll use a small subset. [cite: 351]\n",
    "# To run on the full dataset later, you would comment out these lines. [cite: 352]\n",
    "print(\"Creating a smaller subset for demonstration purposes...\")\n",
    "train_dataset = prepared_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "eval_dataset = prepared_datasets[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# --- To use the full dataset, uncomment the following two lines: ---\n",
    "# train_dataset = prepared_datasets[\"train\"]\n",
    "# eval_dataset = prepared_datasets[\"validation\"]\n",
    "\n",
    "# 3. Initialize and Run the Trainer\n",
    "# The SFTTrainer is a specialized trainer for instruction-formatted datasets. \n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "# 4. Save the Fine-Tuned Artifacts\n",
    "# This saves only the lightweight LoRA adapter, not the full base model. [cite: 381-382]\n",
    "adapter_output_dir = \"./alpacare-lora-adapter\"\n",
    "print(f\"Saving LoRA adapter to {adapter_output_dir}...\")\n",
    "trainer.model.save_pretrained(adapter_output_dir)\n",
    "tokenizer.save_pretrained(adapter_output_dir)\n",
    "print(\"Artifacts saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
